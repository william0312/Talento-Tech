{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S6-MxktWyQrp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f6ab996-8912-4d60-bf7c-c1cfc334a4b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acción seleccionada por el agente: abajo\n"
          ]
        }
      ],
      "source": [
        "#Aprendizaje por refuerzo\n",
        "#Implementación básica de un agente RL:\n",
        "\n",
        "import random\n",
        "\n",
        "class AgenteRL:\n",
        "    def __init__(self, acciones):\n",
        "        self.acciones = acciones\n",
        "\n",
        "    def seleccionar_accion(self, estado):\n",
        "        # Ejemplo de selección aleatoria de acción\n",
        "        return random.choice(self.acciones)\n",
        "\n",
        "# Uso del agente RL\n",
        "acciones_posibles = ['izquierda', 'derecha', 'arriba', 'abajo']\n",
        "agente = AgenteRL(acciones_posibles)\n",
        "estado_actual = [0,0] #Estado inicial del entorno\n",
        "accion_seleccionada = agente.seleccionar_accion(estado_actual)\n",
        "print(\"Acción seleccionada por el agente:\", accion_seleccionada)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EntornoRL:\n",
        "    def __init__(self, estados):\n",
        "        self.estados = estados\n",
        "\n",
        "    def tomar_accion(self,accion):\n",
        "    #Simulación de la transición de estado\n",
        "        nuevo_estado = random.choice(self.estados)\n",
        "        recompensa = random.randint(-10,10)\n",
        "        return nuevo_estado, recompensa\n",
        "\n",
        "#Uso del entorno RL\n",
        "estados_posibles = ['A', 'B', 'C', 'D']\n",
        "entorno = EntornoRL(estados_posibles)\n",
        "accion = 'izquierda' #Acción seleccionada por el agente\n",
        "nuevo_estado, recompensa = entorno.tomar_accion(accion)\n",
        "print(\"Nuevo estado:\", nuevo_estado)\n",
        "print(\"Recompensa recibida:\", recompensa)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OyFJwq60_w3",
        "outputId": "c69fb12a-cb91-4adc-becb-94d8471337f0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nuevo estado: A\n",
            "Recompensa recibida: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementación de un algortimo de Q-Learning\n",
        "\n",
        "class QLearning:\n",
        "    def __init__(self, estados, acciones, alpha=0.1, gamma=0.9,\n",
        "        epsilon=0.1):\n",
        "        self.estados = estados\n",
        "        self.acciones = acciones\n",
        "        self.alpha = alpha #Tasa de aprendizaje\n",
        "        self.gamma = gamma #Factor de descuento\n",
        "        self.epsilon = epsilon #Epsilon-greedy\n",
        "        self.q_table = {}\n",
        "\n",
        "    def actualizar_q_table(self, estado_actual, accion, recompensa, nuevo_estado): #Actualiza la tabla Q\n",
        "        if (estado_actual) not in self.q_table:\n",
        "            self.q_table[(estado_actual)] = {a:0 for a in self.acciones}\n",
        "        if (nuevo_estado) not in self.q_table:\n",
        "            self.q_table[(nuevo_estado)] = {a:0 for a in self.acciones}\n",
        "\n",
        "        q_actual = self.q_table[(estado_actual)][accion]\n",
        "        max_q_nuevo_estado = max(self.q_table[(nuevo_estado)].values())\n",
        "        nuevo_q_valor = q_actual + self.alpha * (recompensa + self.gamma * max_q_nuevo_estado - q_actual)\n",
        "        self.q_table[(estado_actual)][accion] = nuevo_q_valor"
      ],
      "metadata": {
        "id": "_yTCjc642yAt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Uso del algoritmo Q-Learning\n",
        "estados = ['A', 'B', 'C']\n",
        "acciones = ['izquierda', 'derecha']\n",
        "q_learning = QLearning(estados, acciones)\n",
        "\n",
        "#Simulación de una época de entrenamiento\n",
        "estado_actual = 'A'\n",
        "accion = 'izquierda'\n",
        "nuevo_estado = 'B'\n",
        "recompensa = 10\n",
        "q_learning.actualizar_q_table(estado_actual, accion, recompensa, nuevo_estado)\n",
        "\n",
        "#Visualización de la tabla Q\n",
        "print(\"Tabla Q actualizada\")\n",
        "print(\"-------------------\")\n",
        "for estado, acciones in q_learning.q_table.items():\n",
        "    print(f\"{estado}: {acciones}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSLPudHS5P9i",
        "outputId": "82b02cfc-67f8-41b6-a088-8a66edccbd3b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tabla Q actualizada\n",
            "-------------------\n",
            "A: {'izquierda': 1.0, 'derecha': 0}\n",
            "B: {'izquierda': 0, 'derecha': 0}\n"
          ]
        }
      ]
    }
  ]
}