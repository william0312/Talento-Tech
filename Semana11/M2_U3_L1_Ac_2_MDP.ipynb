{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#Proceso de Decisión de Markov (MDP)\n","\n","import numpy as np\n","import random\n","\n","estados = ['A', 'B', 'C']\n","acciones = ['izquierda', 'abajo']\n","recompensas = np.random.randint(0,10,size=(len(estados),len(acciones)))\n","print('recompensas',recompensas)\n","\n","#Función de transición aleatoria\n","def transicion_aleatoria():\n","    return np.random.choice(estados)\n","\n","#Generación de datos\n","estado_actual = np.random.choice(estados)\n","accion = np.random.choice(acciones)\n","nuevo_estado = transicion_aleatoria()\n","recompensa = recompensas[estados.index(estado_actual), acciones.index(accion)]\n","\n","print(\"Estado actual:\", estado_actual)\n","print(\"Acción tomada:\", accion)\n","print(\"Nuevo estado:\", nuevo_estado)\n","print(\"Recompensa:\", recompensa)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v69UCBhm7ODD","outputId":"774876fe-e837-4bf7-e3db-3ce83aa3bf74","executionInfo":{"status":"ok","timestamp":1724388456414,"user_tz":300,"elapsed":16,"user":{"displayName":"Óscar Mora","userId":"07893203967880339454"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["recompensas [[9 6]\n"," [2 6]\n"," [4 4]]\n","Estado actual: C\n","Acción tomada: abajo\n","Nuevo estado: C\n","Recompensa: 4\n"]}]},{"cell_type":"code","source":["# Definición de transiciones aleatorias\n","def generar_transiciones(estados, acciones):\n","    transiciones = {}\n","    for estado in estados:\n","        transiciones[estado] = {}\n","        for accion in acciones:\n","            probabilidades = np.random.rand(len(estados))  # Probabilidades aleatorias\n","            probabilidades /= probabilidades.sum()  # Normalizar para que sumen 1\n","            transiciones[estado][accion] = {nuevo_estado: prob for nuevo_estado, prob in zip(estados, probabilidades)}\n","    return transiciones\n","\n","transiciones = generar_transiciones(estados, acciones)\n","print(transiciones)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dO-9gElpEKgr","outputId":"d23d9ebe-0ccd-456f-9f60-95be4d959835","executionInfo":{"status":"ok","timestamp":1724388456414,"user_tz":300,"elapsed":8,"user":{"displayName":"Óscar Mora","userId":"07893203967880339454"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["{'A': {'izquierda': {'A': 0.34623097426091615, 'B': 0.4164203905588898, 'C': 0.23734863518019408}, 'abajo': {'A': 0.07925690126037975, 'B': 0.21070536351891492, 'C': 0.7100377352207053}}, 'B': {'izquierda': {'A': 0.8625235005582618, 'B': 0.04346053866783104, 'C': 0.09401596077390724}, 'abajo': {'A': 0.38169674142137167, 'B': 0.3078024145709977, 'C': 0.31050084400763056}}, 'C': {'izquierda': {'A': 0.37554049201227274, 'B': 0.16815444182972672, 'C': 0.4563050661580006}, 'abajo': {'A': 0.2709631293788761, 'B': 0.6623978952911058, 'C': 0.066638975330018}}}\n"]}]},{"cell_type":"code","source":["# Clase MDP\n","class MDP:\n","    def __init__(self, estados, acciones, transiciones, recompensas):\n","        self.estados = estados\n","        self.acciones = acciones\n","        self.transiciones = transiciones\n","        self.recompensas = recompensas\n","\n","# Crear la instancia del MDP\n","mdp = MDP(estados, acciones, transiciones, recompensas)\n","\n","def calcular_valor_estado(mdp, gamma=0.9, theta=0.01):\n","    valores = {estado: 0 for estado in mdp.estados}\n","    while True:\n","        delta = 0\n","        for estado in mdp.estados:\n","            valor_previo = valores[estado]\n","            valor_nuevo = 0\n","            for accion in mdp.acciones:\n","                if estado in mdp.transiciones and accion in mdp.transiciones[estado]:\n","                    for nuevo_estado in mdp.transiciones[estado][accion]:  # Iterar solo sobre los estados posibles\n","                        prob_transicion = mdp.transiciones[estado][accion].get(nuevo_estado, 0)\n","                        recompensa = mdp.recompensas[mdp.estados.index(estado)][mdp.acciones.index(accion)]\n","                        valor_nuevo += prob_transicion * (recompensa + gamma * valores[nuevo_estado])\n","            valores[estado] = np.clip(valor_nuevo, -1e10, 1e10)  # Limitar el rango de valores\n","            delta = max(delta, abs(valor_previo - valores[estado]))\n","        if delta < theta:\n","            break\n","    return valores\n","\n","#Ejemplos de uso\n","valores_estados = calcular_valor_estado(mdp)\n","print('Valores de los estados:', valores_estados)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XxiLyKHJ8a7D","outputId":"df339726-436b-4af9-d185-3ee10140bdd8","executionInfo":{"status":"ok","timestamp":1724388456414,"user_tz":300,"elapsed":6,"user":{"displayName":"Óscar Mora","userId":"07893203967880339454"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Valores de los estados: {'A': 10000000000.0, 'B': 10000000000.0, 'C': 10000000000.0}\n"]}]},{"cell_type":"code","source":["#Propiedades de Markov\n","#Escribe una función para verificar si un MDP dado cumple con la propiedad de Markov\n","def verificar_propiedad_markov(mdp):\n","    for estado in mdp.estados:\n","        for accion in mdp.acciones:\n","            suma_probabilidades = sum(mdp.transiciones[estado][accion].values())\n","            if not np.isclose(suma_probabilidades, 1.0):\n","                return False\n","        return True\n","\n","#Ejemplo de uso\n","print('Cumple con la propiedad de Markov:', verificar_propiedad_markov(mdp))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bYalZj7YZW6-","outputId":"73905092-44e5-4b80-cd9a-343b35846ba0","executionInfo":{"status":"ok","timestamp":1724388456414,"user_tz":300,"elapsed":4,"user":{"displayName":"Óscar Mora","userId":"07893203967880339454"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Cumple con la propiedad de Markov: True\n"]}]},{"cell_type":"code","source":["print(type(mdp.recompensas))\n","print(mdp.recompensas)\n","print(mdp.transiciones)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"keela3gycwBy","outputId":"37491fc6-a7a5-41aa-b911-443e616fb278","executionInfo":{"status":"ok","timestamp":1724388456872,"user_tz":300,"elapsed":3,"user":{"displayName":"Óscar Mora","userId":"07893203967880339454"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'numpy.ndarray'>\n","[[9 6]\n"," [2 6]\n"," [4 4]]\n","{'A': {'izquierda': {'A': 0.34623097426091615, 'B': 0.4164203905588898, 'C': 0.23734863518019408}, 'abajo': {'A': 0.07925690126037975, 'B': 0.21070536351891492, 'C': 0.7100377352207053}}, 'B': {'izquierda': {'A': 0.8625235005582618, 'B': 0.04346053866783104, 'C': 0.09401596077390724}, 'abajo': {'A': 0.38169674142137167, 'B': 0.3078024145709977, 'C': 0.31050084400763056}}, 'C': {'izquierda': {'A': 0.37554049201227274, 'B': 0.16815444182972672, 'C': 0.4563050661580006}, 'abajo': {'A': 0.2709631293788761, 'B': 0.6623978952911058, 'C': 0.066638975330018}}}\n"]}]},{"cell_type":"code","source":["#Propiedades de recompensa\n","#Escribe una función para calcular la\n","#recompensa promedio por acción en un MDP.\n","\n","import numpy as np\n","\n","def calcular_recompensa_promedio(mdp):\n","    recompensa_total = 0\n","    total_acciones = 0\n","    for estado in mdp.estados:\n","        for accion in mdp.acciones:\n","            for nuevo_estado in mdp.transiciones[estado][accion]:\n","                indice_estado = mdp.estados.index(estado)\n","                indice_accion = mdp.acciones.index(accion)\n","                recompensa_total += mdp.recompensas[indice_estado, indice_accion]\n","                total_acciones += 1\n","    return recompensa_total / total_acciones if total_acciones > 0 else 0\n","\n","# Ejemplo de uso\n","print('Recompensa promedio por acción:', calcular_recompensa_promedio(mdp))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0WJKrhwqaaHu","outputId":"10eb1a84-b961-4f6d-a24a-806d4a2077d7","executionInfo":{"status":"ok","timestamp":1724388456872,"user_tz":300,"elapsed":2,"user":{"displayName":"Óscar Mora","userId":"07893203967880339454"}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Recompensa promedio por acción: 5.166666666666667\n"]}]}]}