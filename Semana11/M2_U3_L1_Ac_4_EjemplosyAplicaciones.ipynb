{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1iIZKdDEQVLknduum_i0jQQmFUSZoLS3X","timestamp":1724351191028},{"file_id":"1cKexFe6kd7G7C6WD0DddrTEKz6mBQDLa","timestamp":1724328989774},{"file_id":"11rjkadEZhK2W0r4iUu4A9c8hzcdst46i","timestamp":1724245926532}],"authorship_tag":"ABX9TyP2DKxDalSzm43tV19Br/OS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# 1: Implementación de Q-Learning en un entorno de gridworld simple\n","import numpy as np\n","\n","# Definición del gridworld\n","gridworld = np.array([\n","  [-1, -1, -1, 1],\n","  [-1, -1, -1, -1],\n","  [-1, -1, -1, -1],\n","  [-1, -1, -1, -1]\n","])\n","\n","# Define las acciones posibles: arriba, abajo, izquierda, derecha\n","actions = {\n","  'up': (-1, 0),\n","  'down': (1, 0),\n","  'left': (0, -1),\n","  'right': (0, 1)\n","}\n","\n","# Implementación de Q-Learning\n","Q = np.zeros((gridworld.shape[0], gridworld.shape[1], len(actions))) # Initialize Q-table correctly\n","\n","gamma = 0.8\n","alpha = 0.1\n","num_episodes = 1000\n","\n","for _ in range(num_episodes):\n","  estado = (0, 0)\n","  while estado != (0, 3):\n","    action_key = np.random.choice(list(actions.keys())) # Choose a random action key\n","    action = actions[action_key] # Get the action tuple from the dictionary\n","    nueva_fila = estado[0] + action[0]\n","    nueva_columna = estado[1] + action[1]\n","    if 0 <= nueva_fila < gridworld.shape[0] and 0 <= nueva_columna < gridworld.shape[1]:\n","      recompensa = gridworld[nueva_fila, nueva_columna]\n","      nuevo_valor = recompensa + gamma * np.max(Q[nueva_fila, nueva_columna])\n","      action_index = list(actions.keys()).index(action_key) # Get the index of the action\n","      Q[estado[0], estado[1], action_index] = (1 - alpha) * Q[estado[0], estado[1], action_index] + alpha * nuevo_valor # Update Q-value for the estado-action pair\n","      estado = (nueva_fila, nueva_columna)\n","\n","# Muestra la tabla Q-valor aprendida\n","print(\"Tabla Q-valor aprendida: \")\n","print(Q)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cyzx9FyMFUJW","outputId":"f68ab7d3-9f96-40f9-efcb-d10b5d9314dd","executionInfo":{"status":"ok","timestamp":1724352263680,"user_tz":300,"elapsed":1874,"user":{"displayName":"Carlos Diaz","userId":"09094496570476557141"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tabla Q-valor aprendida: \n","[[[ 0.   -1.    0.   -1.  ]\n","  [ 0.   -1.8  -1.   -0.2 ]\n","  [ 0.   -1.16 -1.    1.  ]\n","  [ 0.    0.    0.    0.  ]]\n","\n"," [[-1.   -1.    0.   -1.8 ]\n","  [-1.   -1.8  -1.   -1.16]\n","  [-0.2  -1.8  -1.8  -0.2 ]\n","  [ 1.   -1.   -1.16  0.  ]]\n","\n"," [[-1.   -1.    0.   -1.8 ]\n","  [-1.8  -1.   -1.   -1.8 ]\n","  [-1.16 -1.   -1.8  -1.  ]\n","  [-0.2  -1.   -1.8   0.  ]]\n","\n"," [[-1.    0.    0.   -1.  ]\n","  [-1.8   0.   -1.   -1.  ]\n","  [-1.8   0.   -1.   -1.  ]\n","  [-1.    0.   -1.    0.  ]]]\n"]}]},{"cell_type":"code","source":["# Ejercicio 2: Aplicación del Aprendizaje por Refuerzo en juegos\n","import numpy as np\n","\n","# Definición de las recompensas\n","recompensas = {'ganar': 10, 'perder': -1, 'empatar': 0}\n","\n","# Implementación de Q-Learning para el juego\n","Q = {}\n","\n","def q_learning(estado_actual, accion, nuevo_estado, resultado):\n","  if (estado_actual, accion) not in Q:\n","    Q[(estado_actual, accion)] = np.zeros(len(acciones))\n","    if nuevo_estado not in Q:\n","      Q[nuevo_estado] = np.zeros(len(acciones))\n","      nuevo_valor = recompensas[resultado] + gamma * np.max(Q[nuevo_estado])\n","      Q[estado_actual][accion] = (1 - alpha) * Q[estado_actual][accion] + alpha * nuevo_valor"],"metadata":{"id":"DBUkxz-pMVoj","executionInfo":{"status":"ok","timestamp":1724365918673,"user_tz":300,"elapsed":4,"user":{"displayName":"Carlos Diaz","userId":"09094496570476557141"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# Ejercicio 3: Aplicación del Aprendizaje por Refuerzo en robótica\n","import numpy as np\n","\n","# Definición del entormo de navegación (datos ficticos)\n","entorno = np.array([\n","  [0, 0, 0, 0, 0],\n","  [0, -1, -1, -1, 0],\n","  [0, 0, -1, 0, 0],\n","  [0, -1, -1, -1, 0],\n","  [0, 0, 0, 0, 0]\n","])\n","\n","# Definición de acciones posibles arriba, abajo, izquierda, derecha\n","acciones = {\n","  'up': (0, -1),\n","  'down': (0, 1),\n","  'left': (-1, 0),\n","  'right': (1, 0)\n","}\n","\n","# Implementación de Q-Learning para el entorno de navegación\n","Q = np.zeros((entorno.shape[0], entorno.shape[1], len(acciones)))\n","\n","gamma = 0.9\n","alpha = 0.1\n","num_episodes = 1000\n","\n","for _ in range(num_episodes):\n","  estado = (0, 0)\n","  while True:\n","    # Choose a random action key instead of an index\n","    accion_key = np.random.choice(list(acciones.keys()))\n","    accion = acciones[accion_key] # Get the action tuple using the key\n","    nueva_fila = estado[0] + accion[0]\n","    nueva_columna = estado[1] + accion[1]\n","    if 0 <= nueva_fila < entorno.shape[0] and 0 <= nueva_columna < entorno.shape[1]:\n","      recompensa = entorno[nueva_fila, nueva_columna]\n","      nuevo_valor = recompensa + gamma * np.max(Q[nueva_fila, nueva_columna])\n","      # Get the index of the action to update the corresponding Q-value\n","      accion_index = list(acciones.keys()).index(accion_key)\n","      Q[estado[0], estado[1], accion_index] = (1 - alpha) * Q[estado[0], estado[1], accion_index] + alpha * nuevo_valor\n","      estado = (nueva_fila, nueva_columna)\n","      if recompensa == -1:\n","        break\n","\n","  # Muestra la tabla Q-valor aprendida\n","print(\"Tabla Q-valor aprendida: \")\n","print(Q)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EE9kHHV3OOEq","executionInfo":{"status":"ok","timestamp":1724367411745,"user_tz":300,"elapsed":504,"user":{"displayName":"Carlos Diaz","userId":"09094496570476557141"}},"outputId":"96e7e69c-48d8-477b-e146-d40fcc14e231"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Tabla Q-valor aprendida: \n","[[[ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.         -1.        ]\n","  [ 0.          0.          0.         -0.99999861]\n","  [ 0.          0.          0.         -0.99838269]\n","  [ 0.          0.          0.          0.        ]]\n","\n"," [[ 0.         -1.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [-0.71757046  0.          0.          0.        ]]\n","\n"," [[ 0.          0.          0.          0.        ]\n","  [ 0.         -0.96566316 -0.96566316 -0.97218716]\n","  [ 0.          0.          0.          0.        ]\n","  [-0.3439      0.         -0.19       -0.1       ]\n","  [ 0.          0.          0.          0.        ]]\n","\n"," [[ 0.         -0.9953616   0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [-0.3439      0.          0.          0.        ]]\n","\n"," [[ 0.          0.          0.          0.        ]\n","  [ 0.          0.         -0.81469798  0.        ]\n","  [ 0.          0.         -0.468559    0.        ]\n","  [ 0.          0.         -0.1         0.        ]\n","  [ 0.          0.          0.          0.        ]]]\n"]}]},{"cell_type":"code","source":["# Ejercicio 4: Aplicación del Aprendizaje por Refuerzo en gestión de recursos\n","\n","import numpy as np\n","\n","# Definición de los estados (niveles de inventario), acciones (ordenes de reabastecimiento) y recompensas (costos, ganancias, etc.)\n","estados = ['bajo', 'medio', 'alto']\n","acciones = ['reabastecer', 'no_reabastecer']\n","recompensas = {\n","  ('bajo', 'reabastecer'): -50,\n","  ('bajo', 'no_reabastecer'): -10,\n","  ('medio', 'reabastecer'): 30,\n","  ('medio', 'no_reabastecer'): 0,\n","  ('alto', 'reabastecer'): 10,\n","  ('alto', 'no_reabastecer'): -20\n","}\n","\n","# Implementación de Q-Learning\n","Q = {}\n","\n","gamma = 0.9\n","alpha = 0.1\n","num_episodes = 1000\n","\n","for _ in range(num_episodes):\n","  estado_actual = np.random.choice(estados)\n","  while True:\n","    accion = np.random.choice(acciones)\n","    recompensa = recompensas[(estado_actual, accion)]\n","    if (estado_actual, accion) not in Q:\n","      Q[estado_actual] = {}\n","    if accion not in Q[estado_actual]:\n","      Q[estado_actual][accion] = 0\n","      nuevo_estado = np.random.choice(estados)\n","      max_nuevo_estado = max(Q[nuevo_estado].values()) if nuevo_estado in Q else 0\n","      Q[estado_actual][accion] += alpha * (recompensa + gamma * max_nuevo_estado - Q[estado_actual][accion])\n","      estado_actual = nuevo_estado\n","      if recompensa == 50 or recompensa == 30 or recompensa ==10:\n","        break\n","\n","# Muestra la tabla Q-valor aprendida\n","print(\"Tabla Q-valor aprendida: \")\n","print(Q)\n",""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fXnzh5GUTuHA","executionInfo":{"status":"ok","timestamp":1724368332922,"user_tz":300,"elapsed":219,"user":{"displayName":"Carlos Diaz","userId":"09094496570476557141"}},"outputId":"1720dc3e-8974-4cdc-f370-2c9124f1d4c8"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Tabla Q-valor aprendida: \n","{'medio': {'no_reabastecer': -0.4536493105770001}, 'bajo': {'no_reabastecer': -1.1593745594156737}, 'alto': {'reabastecer': 0.8956562896525894}}\n"]}]}]}