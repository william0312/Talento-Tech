{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":204,"status":"ok","timestamp":1724389004742,"user":{"displayName":"Óscar Mora","userId":"07893203967880339454"},"user_tz":300},"id":"2QQTFtzIzvfD","outputId":"1d3607e4-7f8a-4091-cb1a-d66b4853b9fa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Estados posibles: [0 1 2 3]\n","Acciones posibles: [0 1]\n","Recompensas por estado: {0: -1, 1: -1, 2: -1, 3: 10}\n"]}],"source":["#Ejercicio de Aprendizaje por Refuerzo en Python\n","\n","#Ejercicio 1: Introducción a los principales algoritmos de RL\n","#Define el entorno del juego\n","\n","import numpy as np\n","\n","class Environment:\n","    def __init__(self):\n","        self.state_space = np.array([0, 1, 2, 3]) #Estados posibles\n","        self.action_space = np.array([0, 1]) #Acciones posibles\n","        self.rewards = {0: -1, 1: -1, 2: -1, 3: 10} #Recompensas por estado\n","\n","#Crea una instancia del entorno\n","env = Environment()\n","\n","#Nuestra información del entorno\n","print(\"Estados posibles:\", env.state_space)\n","print(\"Acciones posibles:\", env.action_space)\n","print(\"Recompensas por estado:\", env.rewards)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1002,"status":"ok","timestamp":1724389005742,"user":{"displayName":"Óscar Mora","userId":"07893203967880339454"},"user_tz":300},"id":"HQLX-M-j1US9","outputId":"203a423d-06a9-4d4a-9468-59a6aa972866"},"outputs":[{"name":"stdout","output_type":"stream","text":["Función Q-valor aprendida:\n","[[ 4.58  6.2 ]\n"," [ 6.2   8.  ]\n"," [ 8.   10.  ]\n"," [ 0.    0.  ]]\n"]}],"source":["#Eejercicio: Q-Learning\n","import numpy as np\n","\n","# Inicializa la tabla Q con valores arbitrarios\n","Q = np.zeros((len(env.state_space), len(env.action_space)))\n","\n","#Define los parámetros del algortimo\n","alpha = 0.1 #Tasa de aprendizaje\n","gamma = 0.9 #Factor de descuento\n","\n","#Define los hiperparámetros del algoritmo\n","alpha = 0.1 #Tasa de aprendizaje\n","gamma = 0.9 #Factor de descuento\n","\n","# Entrena el agente utilizando Q-Learning\n","for _ in range(10000):\n","    state = np.random.choice(env.state_space)  # Estado inicial\n","    while state != 3:  # Mientras no lleguemos al estado objetivo\n","        action = np.random.choice(env.action_space)  # Selecciona una acción aleatoria\n","        next_state = state + action  # Estado siguiente\n","\n","        # Verifica que el next_state esté en los límites válidos\n","        if next_state < 0 or next_state >= len(env.state_space):\n","            break  # Salir del bucle si el estado siguiente es inválido\n","\n","        # Verifica si next_state tiene una recompensa definida\n","        if next_state in env.rewards:\n","            reward = env.rewards[next_state]  # Recompensa por el estado siguiente\n","        else:\n","            reward = 0  # Asigna una recompensa por defecto si no está definido\n","\n","        Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n","        state = next_state  # Actualiza el estado\n","\n","#Imprime la tabla Q final\n","print(\"Función Q-valor aprendida:\")\n","print(Q)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":870,"status":"ok","timestamp":1724389006610,"user":{"displayName":"Óscar Mora","userId":"07893203967880339454"},"user_tz":300},"id":"b1r3ydsX66gg","outputId":"3e4bb061-4b14-4167-f9dc-513eb4474ab9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Función Q-valor aprendida con Sarsa:\n","[[ 1.43691561  2.81682996]\n"," [ 2.21990494  5.19547816]\n"," [ 5.96716415 10.        ]\n"," [ 0.          0.        ]]\n"]}],"source":["# Sarsa\n","\n","#Ejercicio 3:Sarsa\n","#Reinicializa la tabla Q con valore arbitrarios\n","Q = np.zeros((len(env.state_space), len(env.action_space)))\n","\n","#Entrena el agente utilizando Sarsa\n","for _ in range(10000):\n","    state = np.random.choice(env.state_space) #Estado inicial\n","    action = np.random.choice(env.action_space) #Selecciona una acción aleatoria\n","    while state != 3: #Mientras no lleguemos al estado objetivo\n","        next_state = state + action #Estado siguiente\n","        next_action = np.random.choice(env.action_space) #Selecciona una acción aleatoria\n","        reward = env.rewards[next_state] #Recompensa por el estado siguiente\n","        Q[state, action] = Q[state, action] + alpha * (reward + gamma * Q[next_state, next_action] - Q[state, action])\n","        state = next_state #Actualiza el estado\n","        action = next_action #Actualiza la acción\n","\n","#Imprime la tabla Q final que aprendió con Sara\n","print(\"Función Q-valor aprendida con Sarsa:\")\n","print(Q)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2144,"status":"ok","timestamp":1724389008752,"user":{"displayName":"Óscar Mora","userId":"07893203967880339454"},"user_tz":300},"id":"TOwxQ0bL8Rkt","outputId":"72ca2462-a554-43c3-88ce-de35469a4b09"},"outputs":[{"name":"stdout","output_type":"stream","text":["Política aprendida por Gradiente de Montecarlo:\n","[[2.78060828e-001 7.21939172e-001]\n"," [5.87240460e-002 9.41275954e-001]\n"," [1.14452174e-315 1.00000000e+000]\n"," [5.00000000e-001 5.00000000e-001]]\n"]}],"source":["#Política con gradiente Monecarlo\n","#Ejercicio 4: Política de Gradiente de Montecarlo\n","# Incializa la política don probabilidades uniformes\n","\n","# Inicializa la política con probabilidades uniformes\n","policy = np.ones((len(env.state_space), len(env.action_space))) / len(env.action_space)\n","\n","# Inicializa la tabla Q\n","Q = np.zeros((len(env.state_space), len(env.action_space)))\n","\n","# Define la función de recompensa promedio\n","def average_reward(Q):\n","    return np.mean([Q[state, np.argmax(policy[state])] for state in env.state_space])\n","\n","# Entrena la política utilizando Gradiente de Montecarlo\n","for _ in range(10000):\n","    state = np.random.choice(env.state_space)  # Estado inicial\n","    while state != 3:  # Mientras no lleguemos al estado objetivo\n","        action = np.random.choice(env.action_space, p=policy[state])  # Selecciona una acción basada en la política\n","        next_state = state + action  # Estado siguiente\n","\n","        # Verifica que el next_state esté dentro de los límites válidos\n","        if next_state < 0 or next_state >= len(env.state_space):\n","            break  # Salir del bucle si el estado siguiente es inválido\n","\n","        reward = env.rewards[next_state]  # Recompensa por el estado siguiente\n","\n","        # Calcula el gradiente\n","        gradient = np.zeros_like(policy[state]) #Calcula el gradiente\n","        gradient[action] = 1\n","\n","        alpha = 0.01  # Tasa de aprendizaje\n","        policy[state] += alpha * gradient * (reward - average_reward(Q))\n","\n","        # Corrige cualquier valor negativo en la política\n","        policy[state] = np.maximum(policy[state], 0)\n","\n","        # Normaliza la política para que las probabilidades sumen 1\n","        policy[state] = policy[state] / np.sum(policy[state])\n","\n","        state = next_state  # Actualiza el estado\n","\n","#Muestra la política aprendida\n","print(\"Política aprendida por Gradiente de Montecarlo:\")\n","print(policy)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}
