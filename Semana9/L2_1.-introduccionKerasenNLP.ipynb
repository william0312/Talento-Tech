{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPFx4X3lmdMTwfJM6nnmbMG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["pip install --upgrade tensorflow\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MriQBDBeg-8z","executionInfo":{"status":"ok","timestamp":1723045514021,"user_tz":300,"elapsed":4249,"user":{"displayName":"William Mora","userId":"14920045433949921219"}},"outputId":"dca2f45e-6725-4d41-8846-b514b2b33161"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.31.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n","Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n","Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.7.1)\n","Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n","Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.6)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"]}]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1FDrKe_ZZ2yb","executionInfo":{"status":"ok","timestamp":1723045518789,"user_tz":300,"elapsed":256,"user":{"displayName":"William Mora","userId":"14920045433949921219"}},"outputId":"675f4515-3e91-4833-c0f3-441e3658efc0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Keras version: 3.4.1\n","TensorFlow version: 2.17.0\n"]}],"source":["\n","\n","import keras\n","import tensorflow as tf\n","\n","print(\"Keras version:\", keras.__version__)\n","print(\"TensorFlow version:\", tf.__version__)\n","\n","frases =['Hola mundo',\n","         'Hola a todos',\n","         'Hola a todo el mundo']\n","\n","\n","\n"]},{"cell_type":"code","source":["\n","#no sirven, toco hacerlo con tensorflow\n","#from keras.preprocessing.text import Tokenizer\n","#from keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","# Genera el diccionario de tokens\n","tokenizer = Tokenizer(num_words=10)\n","tokenizer.fit_on_texts(frases)\n","word_index = tokenizer.word_index\n","print('word_index=', word_index)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P6QShIe8LmzY","executionInfo":{"status":"ok","timestamp":1723045632918,"user_tz":300,"elapsed":254,"user":{"displayName":"William Mora","userId":"14920045433949921219"}},"outputId":"90e61ba0-57df-4119-a51a-10417c692bb1"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["word_index= {'hola': 1, 'mundo': 2, 'a': 3, 'todos': 4, 'todo': 5, 'el': 6}\n"]}]},{"cell_type":"code","source":["#generacion secuencia de tokenizadas\n","\n","secuencias = tokenizer.texts_to_sequences(frases)\n","print('secuencias=', secuencias)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n1ADkiHWhuSC","executionInfo":{"status":"ok","timestamp":1723045958754,"user_tz":300,"elapsed":266,"user":{"displayName":"William Mora","userId":"14920045433949921219"}},"outputId":"48513cb6-403d-466e-eda9-c3987edfeb39"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["secuencias= [[1, 2], [1, 3, 4], [1, 3, 5, 6, 2]]\n"]}]},{"cell_type":"code","source":["#rellena las secuencias a una longitud uniforme\n","\n","relleno= keras.preprocessing.sequence.pad_sequences(secuencias)\n","print('rellena =\\n', relleno)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1KqcsF7mjJxg","executionInfo":{"status":"ok","timestamp":1723046134888,"user_tz":300,"elapsed":245,"user":{"displayName":"William Mora","userId":"14920045433949921219"}},"outputId":"8578a2ca-21ce-485f-a148-658df990f167"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["rellena =\n"," [[0 0 0 1 2]\n"," [0 0 1 3 4]\n"," [1 3 5 6 2]]\n"]}]},{"cell_type":"code","source":["import keras\n","import tensorflow as tf\n","\n","frases =['Hola mundo',\n","         'Hola a todos',\n","         'Hola a todo el mundo',\n","         'Buen dia, como estas hoy']\n","\n","#no sirven, toco hacerlo con tensorflow\n","#from keras.preprocessing.text import Tokenizer\n","#from keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","# Genera el diccionario de tokens\n","tokenizer = Tokenizer(num_words=10)\n","tokenizer.fit_on_texts(frases)\n","word_index = tokenizer.word_index\n","print('word_index=', word_index)\n","\n","#generacion secuencia de tokenizadas\n","\n","secuencias = tokenizer.texts_to_sequences(frases)\n","print('secuencias=', secuencias)\n","\n","#rellena las secuencias a una longitud uniforme\n","\n","relleno= keras.preprocessing.sequence.pad_sequences(secuencias)\n","print('rellena =\\n', relleno)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fePJxhbyjvWw","executionInfo":{"status":"ok","timestamp":1723046523286,"user_tz":300,"elapsed":321,"user":{"displayName":"William Mora","userId":"14920045433949921219"}},"outputId":"61c50924-e113-44dd-ca06-af3e21c3b28c"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["word_index= {'hola': 1, 'mundo': 2, 'a': 3, 'todos': 4, 'todo': 5, 'el': 6, 'buen': 7, 'dia': 8, 'como': 9, 'estas': 10, 'hoy': 11}\n","secuencias= [[1, 2], [1, 3, 4], [1, 3, 5, 6, 2], [7, 8, 9]]\n","rellena =\n"," [[0 0 0 1 2]\n"," [0 0 1 3 4]\n"," [1 3 5 6 2]\n"," [0 0 7 8 9]]\n"]}]},{"cell_type":"code","source":["import keras\n","import tensorflow as tf\n","\n","frases =['Hola mundo',\n","         'Hola a todos',\n","         'Hola a todo el mundo',\n","         'Buen dia, como estas hoy']\n","\n","#no sirven, toco hacerlo con tensorflow\n","#from keras.preprocessing.text import Tokenizer\n","#from keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","# Genera el diccionario de tokens\n","tokenizer = Tokenizer(num_words=10,oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(frases)\n","word_index = tokenizer.word_index\n","print('word_index=', word_index)\n","\n","#generacion secuencia de tokenizadas\n","\n","secuencias = tokenizer.texts_to_sequences(frases)\n","print('secuencias=', secuencias)\n","\n","#rellena las secuencias a una longitud uniforme\n","\n","relleno= keras.preprocessing.sequence.pad_sequences(secuencias)\n","print('rellena =\\n', relleno)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pPrPEPKPk-4F","executionInfo":{"status":"ok","timestamp":1723046612887,"user_tz":300,"elapsed":229,"user":{"displayName":"William Mora","userId":"14920045433949921219"}},"outputId":"69a05f97-461f-410a-e887-0a37a1632b4d"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["word_index= {'<OOV>': 1, 'hola': 2, 'mundo': 3, 'a': 4, 'todos': 5, 'todo': 6, 'el': 7, 'buen': 8, 'dia': 9, 'como': 10, 'estas': 11, 'hoy': 12}\n","secuencias= [[2, 3], [2, 4, 5], [2, 4, 6, 7, 3], [8, 9, 1, 1, 1]]\n","rellena =\n"," [[0 0 0 2 3]\n"," [0 0 2 4 5]\n"," [2 4 6 7 3]\n"," [8 9 1 1 1]]\n"]}]},{"cell_type":"code","source":["import keras\n","import tensorflow as tf\n","\n","frases =['Hola mundo',\n","         'Hola a todos',\n","         'Hola a todo el mundo',\n","         'Buen dia, como estas hoy']\n","\n","#no sirven, toco hacerlo con tensorflow\n","#from keras.preprocessing.text import Tokenizer\n","#from keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","# Genera el diccionario de tokens\n","tokenizer = Tokenizer(num_words=10,oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(frases)\n","word_index = tokenizer.word_index\n","print('word_index=', word_index)\n","\n","#generacion secuencia de tokenizadas\n","\n","secuencias = tokenizer.texts_to_sequences(frases)\n","print('secuencias=', secuencias)\n","\n","\n","#rellena las secuencias a una longitud uniforme\n","\n","relleno= keras.preprocessing.sequence.pad_sequences(secuencias,padding='post',truncating='post')\n","print('rellena =\\n', relleno)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Q74IWRzmvp9","executionInfo":{"status":"ok","timestamp":1723047179547,"user_tz":300,"elapsed":168,"user":{"displayName":"William Mora","userId":"14920045433949921219"}},"outputId":"ba8e2e38-e45c-4537-f3bf-dab6280c9dc7"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["word_index= {'<OOV>': 1, 'hola': 2, 'mundo': 3, 'a': 4, 'todos': 5, 'todo': 6, 'el': 7, 'buen': 8, 'dia': 9, 'como': 10, 'estas': 11, 'hoy': 12}\n","secuencias= [[2, 3], [2, 4, 5], [2, 4, 6, 7, 3], [8, 9, 1, 1, 1]]\n","rellena =\n"," [[2 3 0 0 0]\n"," [2 4 5 0 0]\n"," [2 4 6 7 3]\n"," [8 9 1 1 1]]\n"]}]}]}